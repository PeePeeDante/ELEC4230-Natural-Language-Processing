{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import glob\n",
    "import csv, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = 'bert-base-uncased'\n",
    "model_dir = 'bert-base-uncased'\n",
    "layers = 12\n",
    "preds_dir = \"results/bert-base-uncased/epoch_2\"\n",
    "data_path = \"dev-v2.0.json\"\n",
    "sample_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qid_to_has_ans(dataset):\n",
    "    qid_to_has_ans = {}\n",
    "    for article in dataset:\n",
    "        for p in article['paragraphs']:\n",
    "            for qa in p['qas']: # question, answer, id\n",
    "                qid_to_has_ans[qa['id']] = bool(qa['answers']) # answer: [] -> false; else ->　true\n",
    "    return qid_to_has_ans\n",
    "    # id ->　true/false\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "    # true/ false \n",
    "    # idea:\"i have fat ass\" and \"fat ass\" -> after cleaning results in (\"ass\" == \"ass\") ->　true\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks) # intersect\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0: # intersect is null\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks) # TP / TP+FP\n",
    "    recall = 1.0 * num_same / len(gold_toks) # TP / TP+FN\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def get_raw_scores(dataset, preds):\n",
    "    exact_scores = {}\n",
    "    f1_scores = {}\n",
    "    for article in dataset:\n",
    "        for p in article['paragraphs']:\n",
    "            for qa in p['qas']:\n",
    "                qid = qa['id']\n",
    "                gold_answers = [a['text'] for a in qa['answers'] #'answers': [{'text': '10th and 11th centuries', 'answer_start': 94},\n",
    "                            if normalize_answer(a['text'])] # check if still meaningful after cleaning\n",
    "                if not gold_answers:\n",
    "                  # For unanswerable questions, only correct answer is empty string\n",
    "                    gold_answers = ['']\n",
    "                if qid not in preds:\n",
    "                    # print('Missing prediction for %s' % qid)\n",
    "                    continue\n",
    "                a_pred = preds[qid] # string\n",
    "                # Take max over all gold answers\n",
    "                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers) # gold_answer -> a['text'] list of strings # can have muliple answer\n",
    "                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
    "    return exact_scores, f1_scores\n",
    "\n",
    "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
    "    new_scores = {}\n",
    "    for qid, s in scores.items():\n",
    "        pred_na = na_probs[qid] > na_prob_thresh # na_probs[qid] <- scores\n",
    "        if pred_na:\n",
    "            new_scores[qid] = float(not qid_to_has_ans[qid])\n",
    "        else:\n",
    "            new_scores[qid] = s\n",
    "    return new_scores\n",
    "\n",
    "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
    "    if not qid_list:\n",
    "        total = len(exact_scores)\n",
    "        return collections.OrderedDict([\n",
    "            ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
    "            ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
    "            ('total', total),\n",
    "        ])\n",
    "    else:\n",
    "        total = len(qid_list)\n",
    "        return collections.OrderedDict([\n",
    "            ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
    "            ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
    "            ('total', total),\n",
    "        ])\n",
    "\n",
    "def merge_eval(main_eval, new_eval, prefix): # exact:__,f1:__,total:__\n",
    "    for k in new_eval: # k: exacr:__, f1:__, total:__\n",
    "        main_eval['%s_%s' % (prefix, k)] = new_eval[k] # out_eval three more keys: has_ans_exact, has_ans_f1, has_ans_total\n",
    "\n",
    "def main(data_file, pred_file):\n",
    "    with open(data_file) as f: # load devset\n",
    "        dataset_json = json.load(f)\n",
    "        dataset = dataset_json['data'][0:sample_size-1]\n",
    "    with open(pred_file) as f: # load pred.json\n",
    "        preds = json.load(f)\n",
    "    na_probs = {k: 0.0 for k in preds}\n",
    "    #print(na_probs) # only id, id:0.0\n",
    "  \n",
    "    qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps id to True/False\n",
    "    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v] # for all qid that answers not [] in dataset\n",
    "    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v] # for all qid that answer is [] in dataset\n",
    "\n",
    "    has_ans_qids_in_pred = [qid for qid in has_ans_qids if qid in preds] # dataset not [] intersect pred not []\n",
    "    no_ans_qids_in_pred = [qid for qid in no_ans_qids if qid in preds] # dataset [] intersect pred []\n",
    "  \n",
    "    exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
    "  \n",
    "    exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,1.0)\n",
    "    f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans, 1.0)\n",
    "  \n",
    "    out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "\n",
    "    if has_ans_qids: # if has_ans_qid is not empty ie []\n",
    "        has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids_in_pred) # dataset not [] intersect pred not []\n",
    "        merge_eval(out_eval, has_ans_eval, 'has_ans')\n",
    "  \n",
    "    if no_ans_qids:\n",
    "        no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids_in_pred) # dataset [] intersect pred []\n",
    "        merge_eval(out_eval, no_ans_eval, 'no_ans')\n",
    "\n",
    "    exact, f1 = out_eval['exact'], out_eval['f1']\n",
    "    exact_no_ans, f1_no_ans = out_eval['no_ans_exact'], out_eval['no_ans_f1']\n",
    "    exact_has_ans, f1_has_ans = out_eval['has_ans_exact'], out_eval['has_ans_f1']\n",
    "\n",
    "    return exact, f1, exact_no_ans, f1_no_ans, exact_has_ans, f1_has_ans\n",
    "\n",
    "\n",
    "def convert_preds_to_json(preds_dir):\n",
    "    \n",
    "    csv_file = preds_dir + \"/predict.csv\"\n",
    "\n",
    "\n",
    "    data = {}\n",
    "    with open(csv_file,encoding = 'unicode_escape') as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            id = row['Id']\n",
    "            pred = row['Predicted']\n",
    "            data[id] = pred\n",
    "\n",
    "    x = json.dumps(data)\n",
    "    f = open(preds_dir +\"/predict.json\", \"w\")\n",
    "    f.write(x)\n",
    "    f.close()\n",
    "\n",
    "def evaluate(preds_dir, data_path):\n",
    "    \n",
    "    exact = 0\n",
    "    f1 = 0\n",
    "    exact_no_ans = 0\n",
    "    f1_no_ans = 0\n",
    "    exact_has_ans = 0\n",
    "    f1_has_ans = 0\n",
    "\n",
    "    json_file = preds_dir + \"/predict.json\"\n",
    "\n",
    "    exact, f1, exact_no_ans, f1_no_ans, exact_has_ans, f1_has_ans = main(data_path, json_file)\n",
    "\n",
    "    results = pd.DataFrame({'layer':0, 'exact':exact, 'f1':f1, 'exact_no_ans':exact_no_ans, 'f1_no_ans':f1_no_ans, 'exact_has_ans':exact_has_ans, 'f1_has_ans':f1_has_ans},index = [0])\n",
    "\n",
    "    csv_name = \"results.csv\"\n",
    "\n",
    "    results.to_csv(preds_dir + \"/\" +csv_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert preds dir csv files to json\n",
    "convert_preds_to_json(preds_dir = preds_dir)\n",
    "\n",
    "# evaluate\n",
    "evaluate(preds_dir = preds_dir, data_path = data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
