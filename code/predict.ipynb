{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import BertTokenizer, AutoConfig, AutoModelForQuestionAnswering, squad_convert_examples_to_features\n",
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = 'bert-base-uncased'\n",
    "preds_dir = \"results/bert-base-uncased/epoch_2\"\n",
    "epoch_dir = \"results/bert-base-uncased/epoch_2\"\n",
    "data_dir = ''\n",
    "data_file = \"dev-v2.0.json\"\n",
    "layers = 12\n",
    "batch_size = 8\n",
    "hidden_dim = 768\n",
    "max_seq_length = 384\n",
    "max_answer_length = 17\n",
    "res_size = 3\n",
    "non_linear = \"relu\"\n",
    "project_dim = 200\n",
    "dropout_r = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16/16 [00:02<00:00,  7.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "device = 'cuda'\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_prefix)    \n",
    "\n",
    "# Extract examples\n",
    "processor = SquadV2Processor()\n",
    "dev_examples = processor.get_train_examples(data_dir=data_dir, filename=data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|█| 6078/6078 [01:00<00:00, 100.49\n",
      "add example index and unique id: 100%|█| 6078/6078 [00:00<00:00, 1210550.3\n"
     ]
    }
   ],
   "source": [
    "# Extract dev features\n",
    "print(\"Loading dev features\")\n",
    "dev_features, dev_dataset = squad_convert_examples_to_features(\n",
    "    examples=dev_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=False,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize config and model\n",
    "config = AutoConfig.from_pretrained(model_prefix, output_hidden_states = True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_prefix, config = config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, in_dim, n_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()   \n",
    "        assert in_dim % n_heads == 0\n",
    "        self.d = in_dim//n_heads\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.WQ = nn.Linear(in_dim, self.d * self.n_heads)\n",
    "        self.WK = nn.Linear(in_dim, self.d * self.n_heads)\n",
    "        self.WV = nn.Linear(in_dim, self.d * self.n_heads)\n",
    "        \n",
    "        self.linear = nn.Linear(self.n_heads * self.d, in_dim)\n",
    "        self.layer_norm = nn.LayerNorm(in_dim)\n",
    "        \n",
    "    def forward(self, h): # (8, 384, 200)\n",
    "        batch_size = h.shape[0]\n",
    "        q_s = self.WQ(h).view(batch_size, -1, self.n_heads, self.d).transpose(1, 2) # (8, 4, 384, 50)\n",
    "        k_s = self.WK(h).view(batch_size, -1, self.n_heads, self.d).transpose(1, 2)\n",
    "        v_s = self.WV(h).view(batch_size, -1, self.n_heads, self.d).transpose(1, 2) \n",
    "\n",
    "        scores = torch.matmul(q_s, k_s.transpose(-1, -2)) / np.sqrt(self.d) #(8, 4, 384, 384)\n",
    "        attn = F.softmax(scores, dim=-1) \n",
    "        context = torch.matmul(attn, v_s) #(8, 4, 384, 50)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d)\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + h)\n",
    "        \n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, in_dim, project_dim, p = dropout_r, max_seq = max_seq_length):\n",
    "        super(Adapter, self).__init__()        \n",
    "                \n",
    "        self.project_down = nn.Linear(in_dim, project_dim)\n",
    "        self.project_up = nn.Linear(project_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.batchnorm = nn.BatchNorm1d(max_seq)\n",
    "        self.layernorm = nn.LayerNorm(in_dim, max_seq)\n",
    "        self.attention = MultiHeadAttention(project_dim)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h = self.project_down(h)\n",
    "        h = self.batchnorm(h)\n",
    "        h = self.attention(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.project_up(h)\n",
    "        h = self.layernorm(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class AdapterModel(nn.Module):\n",
    "    def __init__(self, in_dim, project_dim, max_seq=max_seq_length):\n",
    "        super(AdapterModel, self).__init__()\n",
    "        \n",
    "        self.adapter_list = nn.ModuleList([Adapter(in_dim, project_dim) for i in range(12)])\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "        \n",
    "    def forward(self, all_h):\n",
    "        h = torch.zeros(all_h[0].size()).to(device)\n",
    "            \n",
    "        for i in range(12):\n",
    "            h = self.adapter_list[i](all_h[i]+h)\n",
    "        \n",
    "        return self.linear(h).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dev IDs\n"
     ]
    }
   ],
   "source": [
    "# Predict using probes for each epoch directory present\n",
    "adaptor_predict_s = AdapterModel(hidden_dim, project_dim)\n",
    "adaptor_predict_e = AdapterModel(hidden_dim, project_dim)\n",
    "\n",
    "\n",
    "adaptor_predict_s.load_state_dict(torch.load(epoch_dir+\"/_start_idx_per100\", map_location=device))\n",
    "adaptor_predict_e.load_state_dict(torch.load(epoch_dir+\"/_end_idx_per100\", map_location=device))\n",
    "\n",
    "# Extract IDs\n",
    "print(\"Extracting dev IDs\")\n",
    "n = len(dev_examples)\n",
    "q_ids = []\n",
    "for i in range(n):\n",
    "    q_ids.append(dev_examples[i].qas_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dev data loader\n",
    "eval_sampler = SequentialSampler(dev_dataset)\n",
    "eval_dataloader = DataLoader(dev_dataset, sampler = eval_sampler, batch_size = batch_size)\n",
    "\n",
    "pred = pd.DataFrame()\n",
    "pred['Id'] = q_ids\n",
    "pred['Predicted'] = [\"\"] * len(dev_examples)\n",
    "pred['Question'] = [\"\"] * len(dev_examples)\n",
    "pred['Score'] = [0] * len(dev_examples)\n",
    "\n",
    "# List to keep track of how many unique questions we've seen in each df, questions with\n",
    "# contexts longer than max seq len get split into multiple features based on doc_stride\n",
    "# a good alternative we may implement later is recording for all features, then simplifying with groupby and max\n",
    "# e.g. something like df.sort_values('Score', ascending=False).drop_duplicates(['Question'])\n",
    "question_ids = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   0%|                                 | 0/797 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████| 797/797 [20:00<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation batches\n",
    "print(\"Predicting on dev set\")\n",
    "for batch in tqdm(eval_dataloader, desc = \"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "        # ALBERT/BERT/Distilibert forward pass\n",
    "        idx = batch[3]\n",
    "        outputs = model(**inputs)\n",
    "        attention_hidden_states = outputs[2][1:] #(layers, batch_size, max_seq_len, hidden_size)\n",
    "        # Compute prediction on eval indices\n",
    "        for j, index in enumerate(idx):\n",
    "            index = int(index.item())\n",
    "\n",
    "\n",
    "            # Extract tokens for the current batch\n",
    "            tokens = tokenizer.convert_ids_to_tokens(batch[0][j])\n",
    "\n",
    "            # Find where context starts and ends, since we want to predict in context\n",
    "            context_end = int(max_seq_length - torch.argmax(torch.flip(batch[2][j], [0])).item()) - 1\n",
    "            context_start = int(torch.argmax(batch[2][j]).item())\n",
    "            \n",
    "\n",
    "            \n",
    "            # Find the question, starting right after [CLS] and subtracting 1 to chop off the [SEP] token\n",
    "            question_start = 1\n",
    "            question_end = context_start # [SEP]\n",
    "\n",
    "            question = tokenizer.convert_tokens_to_string(tokens[question_start:question_end-1])\n",
    "            \n",
    "#           print(\"attention_hidden_states.size()\",torch.stack(list(attention_hidden_states)).size()) # ([12, 8, 384, 768])\n",
    "            \n",
    "            # to model\n",
    "            inputs = torch.stack(list(attention_hidden_states))[:,j,:,:].unsqueeze(1) # (12, 1, 384, 768)\n",
    "            # parameters\n",
    "            threshold=0\n",
    "\n",
    "            # probe.predict    \n",
    "            seq_len = inputs.size(2)\n",
    "            inputs = inputs.to(device)\n",
    "            adaptor_predict_s.to(device)\n",
    "            adaptor_predict_e.to(device)\n",
    "\n",
    "            adaptor_predict_s.eval()\n",
    "            adaptor_predict_e.eval()\n",
    "\n",
    "            S = adaptor_predict_s\n",
    "            E = adaptor_predict_e\n",
    "\n",
    "            with torch.no_grad():\n",
    "                start_scores = S(inputs) \n",
    "                end_scores = E(inputs) #inputs [12,1,384,768] output [1,384,1]\n",
    "\n",
    "                _,max_start_score_idx = start_scores.squeeze().max(-1)\n",
    "\n",
    "                \n",
    "                #print(\"end_scores:\", end_scores)\n",
    "                _,max_end_score_idx = end_scores.squeeze().max(-1)\n",
    "                #print(\"end_scores.max(-1)\", end_scores.squeeze().max(-1))\n",
    "                #print(\"tokens[max_end_score_idx]\", tokens[int(max_end_score_idx)])\n",
    "#                 print(start_scores.size())\n",
    "                start_null = start_scores[:,:,0,:] #print the first of all 384 scores\n",
    "                #print(\"start_null\",start_null)\n",
    "                end_null = end_scores[:,:,0,:]\n",
    "\n",
    "                \n",
    "                score_null = start_null + end_null\n",
    "\n",
    "                start_best, end_best = context_start, context_start\n",
    "                score_best = start_scores[:, :,start_best,:] + end_scores[:, :,end_best,:]\n",
    "\n",
    "                for start_curr in range(context_start, context_end):\n",
    "                    start_score = start_scores[:, :,start_curr,:]\n",
    "                    \n",
    "                    end_scores_valid = end_scores[:, :,start_curr:min(start_curr+max_answer_length+1, context_end),:]\n",
    "                    #print(\"end_scores_valid\",end_scores_valid)\n",
    "                    #print(\"end_scores_valid.size()\",end_scores_valid.size())\n",
    "                    \n",
    "                    end_score, end_idx = end_scores_valid.squeeze().max(-1)\n",
    "                    #print(\"end_score\",end_score)\n",
    "                    #print(\"end_idx\",end_idx)\n",
    "                    \n",
    "                    end_curr = end_idx+start_curr\n",
    "                    score_curr = start_score + end_score\n",
    "                    if score_curr >= score_best:\n",
    "                        score_best = score_curr\n",
    "                        start_best, end_best = start_curr, end_curr\n",
    "\n",
    "                non_null_more_likely_than_null = score_best >= (score_null+threshold)\n",
    "\n",
    "                # Multiply by mask to force idx where null is more probable to zero\n",
    "                score = non_null_more_likely_than_null*score_best+(~non_null_more_likely_than_null)*score_null\n",
    "                start_idx = non_null_more_likely_than_null*start_best\n",
    "                end_idx = non_null_more_likely_than_null*end_best\n",
    "\n",
    "            score, start_idx, end_idx = score.cpu().numpy(), start_idx.cpu().numpy(), end_idx.cpu().numpy()\n",
    "\n",
    "            # end of probe.predict\n",
    "            \n",
    "\n",
    "            \n",
    "            start_idx = int(start_idx[0])\n",
    "            end_idx = int(end_idx[0]) \n",
    "            \n",
    "\n",
    "            # Extract predicted answer, converting start tokens to empty strings (no answer)\n",
    "            answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx + 1])\n",
    "\n",
    "            if answer == '[CLS]':\n",
    "                answer = ''\n",
    "\n",
    "            # Check if the question is the same as the last one, if it is go back to the last question id and keep the higher score.\n",
    "            # If the question is not already in the dataframe, then assign it to the dataframe.\n",
    "            # Note we first handle the case where there are no prior questions by storing since we know there are no duplicates\n",
    "            \n",
    "            if question_ids == 0:\n",
    "                pred.loc[question_ids, 'Question'] = question\n",
    "                pred.loc[question_ids, 'Predicted'] = answer\n",
    "                pred.loc[question_ids, 'Score'] = score\n",
    "            \n",
    "            elif (pred.loc[ int(question_ids-1), 'Question'] == question):\n",
    "                question_ids -= 1  \n",
    "                old_score = pred.loc[question_ids, 'Score'] \n",
    "                if score > old_score:\n",
    "                    pred.loc[question_ids, 'Predicted'] = answer\n",
    "                    pred.loc[question_ids, 'Score'] = score\n",
    "            else:\n",
    "                pred.loc[question_ids, 'Question'] = question\n",
    "                pred.loc[question_ids, 'Predicted'] = answer\n",
    "                pred.loc[question_ids, 'Score'] = score\n",
    "            # Increment to new question id (note, for duplicate answers this gets us back to where we were)\n",
    "            question_ids += 1\n",
    "# Save predictions for each layer\n",
    "print(\"Saving predictions\")\n",
    "pred[['Id', 'Predicted']].to_csv(preds_dir + \"/predict.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
